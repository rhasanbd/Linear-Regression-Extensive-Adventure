{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression - Boston Housing Dataset - Regularized Polynomial OLS\n",
    "\n",
    "We will perform **regularized polynomial linear regression** using the sklearn's regularized OLS Linear Regression model.\n",
    "\n",
    "### Polynomial Regression\n",
    "If our data is actually more complex than a simple straight line, we can still use a linear model to fit nonlinear data. A simple way to do this is to add powers of each feature as new features, then train a linear model on this extended set of features. This technique is called Polynomial Regression.\n",
    "\n",
    "Scikit-Learn’s PolynomialFeatures class to transform our training data, adding the higher-degree polynomial of each feature in the training set as new features. PolynomialFeatures also adds all combinations of features up to the given degree.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "URL: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html#sklearn.datasets.load_boston\n",
    "\n",
    "This dataset provides housing values in suburbs of Boston.\n",
    "\n",
    "The **MEDV** variable is the target variable.\n",
    "\n",
    "### Data description\n",
    "\n",
    "The Boston data frame has 506 rows and 14 columns.\n",
    "\n",
    "This data frame contains the following columns:\n",
    "\n",
    "- CRIM: per capita crime rate by town.\n",
    "\n",
    "- ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "\n",
    "- INDUS: proportion of non-retail business acres per town.\n",
    "\n",
    "- CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
    "\n",
    "- NOX: nitrogen oxides concentration (parts per 10 million).\n",
    "\n",
    "- RM: average number of rooms per dwelling.\n",
    "\n",
    "- AGE: proportion of owner-occupied units built prior to 1940.\n",
    "\n",
    "- DIS: weighted mean of distances to five Boston employment centres.\n",
    "\n",
    "- RAD: index of accessibility to radial highways.\n",
    "\n",
    "- TAX: full-value property-tax rate per $10,000.\n",
    "\n",
    "- PTRATIO: pupil-teacher ratio by town.\n",
    "\n",
    "- B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n",
    "\n",
    "- LSTAT: lower status of the population (percent).\n",
    "\n",
    "- MEDV: median value of owner-occupied homes in $1000s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "First load the data and explore the feature names, target names, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13) (506,)\n",
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "boston = datasets.load_boston()\n",
    "print(boston.data.shape, boston.target.shape)\n",
    "print(boston.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create A DataFrame Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(boston.data,columns=boston.feature_names)\n",
    "df = pd.concat([df,pd.Series(boston.target,name='MEDV')],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Check of the Data\n",
    "\n",
    "Let’s take a look at the top five rows using the DataFrame’s head() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the Data\n",
    "\n",
    "DataFrame’s info() method is useful to get a quick description of the data, in particular the total number of rows, and each attribute’s type and number of non-null values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      "CRIM       506 non-null float64\n",
      "ZN         506 non-null float64\n",
      "INDUS      506 non-null float64\n",
      "CHAS       506 non-null float64\n",
      "NOX        506 non-null float64\n",
      "RM         506 non-null float64\n",
      "AGE        506 non-null float64\n",
      "DIS        506 non-null float64\n",
      "RAD        506 non-null float64\n",
      "TAX        506 non-null float64\n",
      "PTRATIO    506 non-null float64\n",
      "B          506 non-null float64\n",
      "LSTAT      506 non-null float64\n",
      "MEDV       506 non-null float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 55.4 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Data: Describe Numerical Attributes\n",
    "\n",
    "DataFrame's describe() method shows a summary of the numerical attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.593761</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.596783</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.647423</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.593761   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.596783   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.647423   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT        MEDV  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Separate Feature Set (Data Matrix X) and Target (1D Vector y)\n",
    "\n",
    "Create a data matrix (X) that contains all features and a 1D target vector (y) containing the target.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# It contains all attributes (all features and the target)\n",
    "allData = df\n",
    "\n",
    "# Use the following code to select a subset of the features, e.g., \"LSTAT\"\n",
    "#X = df[['LSTAT', 'RM']]\n",
    "\n",
    "\n",
    "# Use the following code to select ALL features\n",
    "X = df.drop(columns='MEDV')  # Data Matrix containing all features excluding the target\n",
    "\n",
    "\n",
    "y = df['MEDV'] # 1D targer vector\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Scale The Features\n",
    "\n",
    "We should ensure that all features have a similar scale. Otherwise optimization algorithms (e.g., Gradient Descent based algorithms) will take much longer time to converge.\n",
    "\n",
    "Also, regularization techniques are sensitive to the scale of data. Thus, we must scale the features before applying regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing Model Complexity in High-Degree Polynomial Regression\n",
    "\n",
    "If we have non-linear relationship between the input variables (feature) and the output variable (target), then we have to use high-degree Polynomial Regression. The degree of a Polynomial Regression determines the complexity of the model.\n",
    "\n",
    "\n",
    "The problem with the high-degree Polynomial Regression model is that it may severely overfit the training data. In other words, our Polynomial model will have high variance. On the other hand, the linear model may underfit it (high bias). In general we won’t know what function (the exact degree of the polynomial) generated the data. Then, how can we decide how complex (polynomial degree) our model should be? How can we tell that our model is overfitting or underfitting the data?\n",
    "\n",
    "There are two approaches to make an optimal tradeoff between overfitting (high variance) and underfitting (high bias).\n",
    "- <font color=red>Frequentist Learning (MLE)</font>: Using cross-validation learn the optimal degree (model complexity) that produces best generalization\n",
    "- <font color=red>Bayesian Learning (MAP or regularized regression)</font>: Using cross-validation learn the optimal regularization (penalty) coefficients that produce best generalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequentist Learning Approach (MLE) for Bias-Variance Tradeoff\n",
    "\n",
    "## Using cross-validation learn the optimal degree (model complexity) that produces best generalization\n",
    "\n",
    "\n",
    "In this notebook, we vary the degree of the polynomial and train the Linear Regression model using the training data. Then, compute the mean squared error (mse) for the **test data** using the models with varying degree.\n",
    "\n",
    "Finally, we plot the root mean square error (rmse) values against the varying degree. From this plot we find the optimal degree (that gives the smallest rmse).\n",
    "\n",
    "\n",
    "## <font color=red> Note for Assignment:\n",
    "\n",
    "For the assignment you will have to use cross-validation to get an estimate of a polynomial model’s generalization performance. \n",
    "\n",
    "You will write a function to plot the training and validation root mean square error values (rmse) for the data matrix X with polynomial degree starting from 1 up to a max. polynomial degree. It takes the feature matrix X (usually the training data) and the max polynomial degree; and by using cross-validation computes the average mse values for the training data and the validation data for each degree polynomial X between 1 and the degree set by max polynomial degree. For example, if max polynomial degree = 3, then it will iterate 3 times and in each iteration it will use the data matrix X with polynomial degree 1, 2 and 3, respectively. During each iteration it will use cross-validation to create the training fold and the validation fold. Then, it will compute the average mse values for both the training and the validation fold. Finally, the function will plot the root-mean-square error (rmse) values for the training and validation folds for each degree of the data matrix starting from 1 up to the max polynomial degree.\n",
    "\n",
    "\n",
    "## Model's Complexity: Overfitting or Underfitting\n",
    "If a model performs well on the training data but generalizes poorly (on the validation data) according to the cross-validation metrics, then your model is overfitting. If it performs poorly on both, then it is underfitting. This is one way to tell when a model is too simple or too complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Optimal Degree of the Polynomial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAGDCAYAAABjkcdfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngUKYiIiIiAcKYSIiIiIe/D8UqtNy/ZjHagAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "degreeList = [1,2,3]\n",
    "\n",
    "mse_train, mse_test = [], []\n",
    "\n",
    "for degree in degreeList:\n",
    "\n",
    "    model = make_pipeline(PolynomialFeatures(degree, include_bias=False), StandardScaler(), LinearRegression()) \n",
    "  \n",
    "    model.fit(X_train, y_train)\n",
    "       \n",
    "    # Make prediction \n",
    "    y_train_predicted = model.predict(X_train)\n",
    "    y_test_predicted = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    mse_train.append(mean_squared_error(y_train, y_train_predicted))\n",
    "    mse_test.append(mean_squared_error(y_test, y_test_predicted))\n",
    "   \n",
    "\n",
    "plt.figure(figsize=(10, 6))   \n",
    "plt.plot(degreeList, np.sqrt(mse_test), \"ro-\", alpha=1.0, linewidth=1.0, label=\"Test RMSE\")\n",
    "plt.plot(degreeList, np.sqrt(mse_train), \"bo-\", alpha=1.0, linewidth=1.0, label=\"Train RMSE\") \n",
    "plt.legend(loc=\"best\", fontsize=14) \n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"RMSE for Varying Degree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Learning Approach (MAP) for Bias-Variance Tradeoff\n",
    "\n",
    "#### Using cross-validation learn the optimal regularization (penalty) coefficients that produce best generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized (Penalized) Regression\n",
    "\n",
    "In regularized Regression we regularize (penalize) the model parameters that allow complex models (high-degree Polynomial Regression model) to be trained on datasets of limited size without severe overfitting. It essentially limits the effective model complexity. \n",
    "\n",
    "Thus, the problem of determining the optimal model complexity (degree of polynomial or the basis function) is shifted from one of finding the appropriate number of basis functions (that we did in the Frequentist approach) to one of determining a suitable value of the regularization coefficient.\n",
    "\n",
    "\n",
    "A model with some regularization typically performs better than a model without any regularization. Thus, we should generally prefer regularized Regression over plain Linear Regression.\n",
    "\n",
    "Moreover, the Normal Equation requires computing the inverse of a matrix, but that matrix is not always invertible. In contrast, the matrix for regularized Regression (e.g., Ridge Regression) is always invertible.\n",
    "\n",
    "There are three different regularized OLS models.\n",
    "\n",
    "- Ridge Regression ($l_2$ norm)\n",
    "- Lasso Regression ($l_1$ norm)\n",
    "- Elastic Net (it combines $l_1$ and $l_2$ priors as regularizer)\n",
    "\n",
    "We will use the OLS Ridge and Lasso Regression algorithms on the high-degree polynomial dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "We will implement the Ridge Regression with Scikit-Learn using a closed-form solution. It uses a matrix factorization technique by André-Louis **Cholesky**. However, there are other solvers that we can choose from.\n",
    "\n",
    "We need to find optimal values for the following **two hyper-parameters** of the Ridge regression model.\n",
    "\n",
    "- alpha : {float, array-like}, shape (n_targets)\n",
    "Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization.\n",
    "\n",
    "- solver : {‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’, ‘sag’, ‘saga’}\n",
    "\n",
    "The solvers:\n",
    "1. ‘svd’ uses a Singular Value Decomposition of X to compute the Ridge coefficients. More stable for singular matrices than ‘cholesky’.\n",
    "2. ‘cholesky’ uses the standard scipy.linalg.solve function to obtain a closed-form solution.\n",
    "3. ‘sparse_cg’ uses the conjugate gradient solver as found in scipy.sparse.linalg.cg. As an iterative algorithm, this solver is more appropriate than ‘cholesky’ for large-scale data (possibility to set tol and max_iter).\n",
    "4. ‘lsqr’ uses the dedicated regularized least-squares routine scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative procedure.\n",
    "5. ‘sag’ uses a Stochastic Average Gradient descent, and ‘saga’ uses its improved, unbiased version named SAGA. Both methods also use an iterative procedure, and are often faster than other solvers when both n_samples and n_features are large. Note that ‘sag’ and ‘saga’ fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing.\n",
    "6. ‘auto’ chooses the solver automatically based on the type of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection: Hyperparameter Tunining\n",
    "\n",
    "\n",
    "We need to select the best model based on the optimal values of these hyperparameters. This process is called hyperparameter tuning.\n",
    "\n",
    "The best way to do hyperparameter tuning is to use **cross-validation**.\n",
    "\n",
    "We will use Scikit-Learn’s GridSearchCV to search the combinations of hyperparameter values that provide best performance.\n",
    "\n",
    "We need to tell which hyperparameters we want the GridSearchCV to experiment with, and what values to try out. It will evaluate all the possible combinations of hyperparameter values, using cross-validation. \n",
    "\n",
    "\n",
    "### Important:\n",
    "\n",
    "The GridSearchCV takes an argument to define the scoring metric (performance measure). \n",
    "\n",
    "See the list of possible scoring functions:\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "For regression, we may use \"neg_mean_squared_error\" or \"explained_variance\" scoring function. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a High-Degree Polynomial Data Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Variable that speficies the degree of the polynomial to be added to the feature vector\n",
    "poly_degree = 2\n",
    "\n",
    "\n",
    "# Add polynomial terms with the feature vector using the sklearn PolynomialFeatures class\n",
    "poly_features = PolynomialFeatures(degree=poly_degree, include_bias=False)\n",
    "X_train_poly = poly_features.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection Via Grid Search: Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (negative mean squared error): -13.676251\n",
      "Optimal Hyperparameter Values:  {'solver': 'saga', 'alpha': 0.16631578947368422}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'alpha': np.linspace(0.01, 1.0, num=20), \n",
    "              'solver': [\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"saga\"]}\n",
    "\n",
    "ridge = Ridge()\n",
    "\n",
    "ridge_cv = GridSearchCV(ridge, param_grid, scoring='neg_mean_squared_error', cv=10)\n",
    "ridge_cv.fit(X_train_poly, y_train)\n",
    "\n",
    "print(\"Best Score (negative mean squared error): %f\" % ridge_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", ridge_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select The Best Model for the Ridge Regressor\n",
    "\n",
    "Using the optimal hyperparameter values, create the best model.\n",
    "Then, fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha:  0.16631578947368422\n",
      "Optimal alpha:  saga\n",
      "\n",
      "----------------------------- Model Evaluation -----------------------------\n",
      "\n",
      "Mean squared error: 8.38\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.90\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "\n",
    "\n",
    "# Optimal model parameters\n",
    "ridge_alpha = ridge_cv.best_params_['alpha']\n",
    "print(\"Optimal alpha: \", ridge_alpha)\n",
    "\n",
    "ridge_solver = ridge_cv.best_params_['solver']\n",
    "print(\"Optimal alpha: \", ridge_solver)\n",
    "\n",
    "\n",
    "# Create Ridge linear regression object\n",
    "lin_reg_ridge = Ridge(alpha=ridge_alpha, solver=ridge_solver)\n",
    "\n",
    "# Train the model\n",
    "lin_reg_ridge.fit(X_train_poly, y_train)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Model Evaluation -----------------------------\")\n",
    "\n",
    "# Make prediction \n",
    "y_train_predicted_ridge = lin_reg_ridge.predict(X_train_poly)\n",
    "\n",
    "\n",
    "print(\"\\nMean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train, y_train_predicted_ridge))\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % r2_score(y_train, y_train_predicted_ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model Using Test Data: Ridge Regression\n",
    "\n",
    "We will use the optimal degree for the polynomial to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------- Test Model Evaluation -----------------------------\n",
      "\n",
      "Mean squared error: 12.95\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.82\n"
     ]
    }
   ],
   "source": [
    "# Add polynomial and bias term with the feature vector using the sklearn PolynomialFeatures class\n",
    "poly_features = PolynomialFeatures(degree=poly_degree, include_bias=False)\n",
    "X_test_poly = poly_features.fit_transform(X_test)\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Test Model Evaluation -----------------------------\")\n",
    "\n",
    "# Make prediction \n",
    "y_test_poly_predicted = lin_reg_ridge.predict(X_test_poly)\n",
    "\n",
    "\n",
    "ridge_test_mse = mean_squared_error(y_test, y_test_poly_predicted)\n",
    "\n",
    "print(\"\\nMean squared error: %.2f\" % ridge_test_mse)\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f' % r2_score(y_test, y_test_poly_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a High-Degree Polynomial Data Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variable that speficies the degree of the polynomial to be added to the feature vector\n",
    "poly_degree = 2\n",
    "\n",
    "\n",
    "# Add polynomial terms with the feature vector using the sklearn PolynomialFeatures class\n",
    "poly_features = PolynomialFeatures(degree=poly_degree, include_bias=False)\n",
    "X_train_poly = poly_features.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection Via Grid Search: Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (negative mean squared error): -12.251929\n",
      "Optimal Hyperparameter Values:  {'alpha': 15.714285714285715}\n"
     ]
    }
   ],
   "source": [
    "#param_grid = {'alpha': np.linspace(10.0, 20.0, num=10)}\n",
    "param_grid = {'alpha': np.linspace(10.0, 20.0)}\n",
    "\n",
    "lasso = Lasso()\n",
    "\n",
    "lasso_cv = GridSearchCV(ridge, param_grid, scoring='neg_mean_squared_error', cv=10)\n",
    "lasso_cv.fit(X_train_poly, y_train)\n",
    "\n",
    "print(\"Best Score (negative mean squared error): %f\" % lasso_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", lasso_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select The Best Model for the Lasso Regressor\n",
    "\n",
    "Using the optimal hyperparameter values, create the best model.\n",
    "Then, fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha:  15.714285714285715\n",
      "\n",
      "----------------------------- Model Evaluation -----------------------------\n",
      "\n",
      "Mean squared error: 6.58\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.92\n"
     ]
    }
   ],
   "source": [
    "# Lasso Regression\n",
    "\n",
    "# Optimal model parameters\n",
    "lasso_alpha = lasso_cv.best_params_['alpha']\n",
    "print(\"Optimal alpha: \", lasso_alpha)\n",
    "\n",
    "\n",
    "# Create Lasso linear regression object\n",
    "lin_reg_lasso = Ridge(alpha=lasso_alpha)\n",
    "\n",
    "# Train the model\n",
    "lin_reg_lasso.fit(X_train_poly, y_train)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Model Evaluation -----------------------------\")\n",
    "\n",
    "# Make prediction \n",
    "y_train_predicted_lasso = lin_reg_lasso.predict(X_train_poly)\n",
    "\n",
    "\n",
    "print(\"\\nMean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train, y_train_predicted_lasso))\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % r2_score(y_train, y_train_predicted_lasso))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model Using Test Data: Lasso Regression\n",
    "\n",
    "We will use the optimal degree for the polynomial to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------- Test Model Evaluation -----------------------------\n",
      "\n",
      "Mean squared error: 13.25\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.82\n"
     ]
    }
   ],
   "source": [
    "# Add polynomial and bias term with the feature vector using the sklearn PolynomialFeatures class\n",
    "poly_features = PolynomialFeatures(degree=poly_degree, include_bias=False)\n",
    "X_test_poly = poly_features.fit_transform(X_test)\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Test Model Evaluation -----------------------------\")\n",
    "\n",
    "# Make prediction \n",
    "y_test_poly_predicted = lin_reg_lasso.predict(X_test_poly)\n",
    "\n",
    "lasso_test_mse = mean_squared_error(y_test, y_test_poly_predicted)\n",
    "\n",
    "print(\"\\nMean squared error: %.2f\" % lasso_test_mse)\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f' % r2_score(y_test, y_test_poly_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Ridge Regression vs. Lasso Regression\n",
    "\n",
    "\n",
    "Lasso Regression uses an $l_1$ penalty, which tends to push the weights down to exactly zero. This leads to **<font color=red>sparse models</font>**, where all weights are zero except for the most important weights. This is a way to perform **feature selection** automatically, which is good if we suspect that only a few features actually matter. When we are not sure, we should prefer Ridge Regression.\n",
    "\n",
    "\n",
    "## Comparison of the Weight Values: Ridge Regression vs. Lasso Regression\n",
    "\n",
    "We will see that when the regularization coefficient (alpha) is large (e.g., $ \\geq 1.0$), Lasso regression tends to drive the weight values towards 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ridge (alpha = 0.166)       Lasso (alpha = 15.714)\n",
      "         MSE = 12.949               MSE = 13.250\n",
      "________________________________________________________________\n",
      "\n",
      "      -0.182093                       -0.120149\n",
      "       0.048256                        0.040756\n",
      "      -0.268296                       -0.053346\n",
      "       0.078935                        0.088262\n",
      "      -0.524401                       -0.733751\n",
      "       3.012863                        3.279943\n",
      "      -0.545002                       -0.833058\n",
      "      -0.604065                       -1.381206\n",
      "       0.217359                        0.635861\n",
      "      -0.563726                       -0.833628\n",
      "      -0.958384                       -0.763431\n",
      "       0.341393                        0.529118\n",
      "      -2.584981                       -2.928048\n",
      "       0.083359                        0.054216\n",
      "       0.066984                        0.043138\n",
      "       0.004345                        0.085805\n",
      "       0.583075                        1.684771\n",
      "      -0.230058                       -0.532703\n",
      "       0.417580                        0.710538\n",
      "       0.067385                        0.141625\n",
      "       0.334020                        0.365700\n",
      "      -0.397116                       -0.579515\n",
      "      -0.028199                        0.140766\n",
      "       0.113421                       -0.008890\n",
      "      -0.122730                       -0.150544\n",
      "      -0.110360                        0.667505\n",
      "       0.211972                        0.241432\n",
      "      -0.015777                       -0.101615\n",
      "       0.262368                        0.218197\n",
      "       0.190165                        0.166492\n",
      "       0.075956                       -0.024918\n",
      "      -0.098792                        0.001801\n",
      "      -0.007106                       -0.289332\n",
      "      -0.020318                       -0.266312\n",
      "       0.011299                        0.726373\n",
      "       0.209725                        0.199085\n",
      "      -0.034756                       -0.060089\n",
      "       0.123161                       -0.493615\n",
      "       0.488926                        0.702415\n",
      "       0.230107                        0.043088\n",
      "       0.150337                        0.623924\n",
      "      -0.060435                        0.700250\n",
      "       0.499029                        0.554144\n",
      "       0.254616                        0.729121\n",
      "       0.149722                        0.016309\n",
      "       0.198634                        0.292271\n",
      "      -0.371167                       -0.524731\n",
      "       0.022245                        0.121045\n",
      "      -0.564128                       -0.614263\n",
      "       0.268047                        0.299720\n",
      "      -0.848269                       -1.037416\n",
      "      -0.639512                       -0.753992\n",
      "       0.284388                        0.174636\n",
      "      -0.314813                       -0.495566\n",
      "       0.304036                        0.245063\n",
      "       0.537772                        0.568534\n",
      "       0.074664                       -0.125540\n",
      "      -0.125922                       -0.117026\n",
      "      -0.564283                       -0.448760\n",
      "      -0.445651                       -0.453845\n",
      "      -0.837945                       -0.808758\n",
      "      -0.014879                       -0.250209\n",
      "       0.476327                        0.687657\n",
      "      -0.429610                       -0.627660\n",
      "      -0.047234                        0.088618\n",
      "      -0.134579                       -0.467084\n",
      "      -0.146076                       -0.148155\n",
      "       0.174725                        0.501944\n",
      "       0.490023                        0.522248\n",
      "      -0.334423                       -0.671370\n",
      "       0.062969                        0.025098\n",
      "      -0.836588                       -1.139635\n",
      "      -0.600291                       -0.915650\n",
      "      -0.948170                       -1.021219\n",
      "      -0.230884                       -0.398365\n",
      "      -0.338677                       -0.061735\n",
      "       0.209027                        0.097216\n",
      "      -0.101480                        0.214552\n",
      "       0.418390                        0.865181\n",
      "       0.422812                        0.255949\n",
      "      -0.149915                        0.008599\n",
      "      -0.304518                       -0.707042\n",
      "      -0.808657                       -1.064885\n",
      "       0.286438                        0.950441\n",
      "       0.191604                       -0.027196\n",
      "       0.089771                       -0.049557\n",
      "       0.196213                       -0.121204\n",
      "      -0.206520                       -0.282686\n",
      "       0.745348                        0.686299\n",
      "      -0.143515                       -0.717536\n",
      "       0.414464                        0.702541\n",
      "       0.558465                        0.413180\n",
      "      -0.013579                       -0.028348\n",
      "      -0.726715                       -1.343076\n",
      "       0.349771                        0.241930\n",
      "       0.482576                        1.155462\n",
      "      -0.139970                       -0.421468\n",
      "      -0.766621                       -1.047801\n",
      "      -0.219884                       -0.162316\n",
      "      -0.088271                        0.076339\n",
      "       0.034789                        0.176694\n",
      "      -0.350265                       -0.310684\n",
      "      -0.468112                       -0.143151\n",
      "       0.716853                        0.966653\n"
     ]
    }
   ],
   "source": [
    "print(\"%8s (alpha = %3.3f)  %10s (alpha = %3.3f)\" % (\"Ridge\",ridge_alpha, \"Lasso\", lasso_alpha))\n",
    "print(\"%8s MSE = %3.3f  %12s MSE = %3.3f\" % (\" \",ridge_test_mse, \" \", lasso_test_mse))\n",
    "print(\"________________________________________________________________\\n\")\n",
    "for i in range(lin_reg_ridge.coef_.shape[0]):\n",
    "    print(\"%15f  %30f\" % (lin_reg_ridge.coef_[i], lin_reg_lasso.coef_[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Note: Regularized Polynomial Regression\n",
    "\n",
    "We observe that only by increasing model complexity (higher-degree polynomial) and regularizing its weights, we could improve performance on the test data for the given dataset."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
