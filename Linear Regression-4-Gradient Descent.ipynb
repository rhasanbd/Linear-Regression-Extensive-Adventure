{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression - Gradient Descent\n",
    "\n",
    "\n",
    "In this notebook we investigate an iterative optimization method known as Gradient Descent (GD) to solve the Linear Regression problem. We will use the sklearn's Gradient Descent model.\n",
    "\n",
    "\n",
    "<font color=red size=3> **Application Scenario:**</font> \n",
    "The GD approach is suitable for the following scenario.\n",
    "- Dataset: can be very large large (unable to fit into computer's memory)\n",
    "- No. of Features: large\n",
    "- Relationship between input (features) and output (target): Linear & Non-Linear (Polynomial Regression)\n",
    "- Out-of-core support: Out-of-core algorithms can handle vast quantities of data that cannot fit in a computer’s main memory. An out-of-core learning algorithm chops the data into mini-batches and uses online learning techniques to learn from these mini-batches.\n",
    "\n",
    "\n",
    "## Tasks\n",
    "We perform the following tasks.\n",
    "- Task 1: Implement GD on a Linear Regression Model\n",
    "- Task 2: Implement GD on a Polynomial Regression Model\n",
    "\n",
    "We will see that the Polynomial Regression model has more power to discover the non-linear pattern in the dataset, however, it overfits (high-variance and less generalizable). By implementing a regularized Polynomial Regression model using GD we can obtain better generalizable solution. Note that we obtained similar optimal solution in notebook 3 by using the OLS regularized Polynimial Regression model.\n",
    "\n",
    "\n",
    "\n",
    "## Gradient Descent\n",
    "The gradient descent based methods are used when there are a large number of features, or too many training instances to fit in memory.\n",
    "\n",
    "There are three variants of the Gradient Descent Algorithms.\n",
    "- Batch Gradient Descent\n",
    "- Stochastic Gradient Descent (SGD)\n",
    "- Mini-batch Gradient Descent\n",
    "\n",
    "Scikit-Learn provides only the Stochastic Gradient Descent model. In this notebook, we will see how to use sklearn's SGDRegressor.\n",
    "\n",
    "\n",
    "### Batch Gradient Descent\n",
    "Batch Gradient Descent uses the whole batch of training data at every step. As a result it is terribly slow on very large training sets.\n",
    "\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "\n",
    "The main problem with Batch Gradient Descent is the fact that it uses the whole training set to compute the gradients at every step, which makes it very slow when the training set is large. At the opposite extreme, Stochastic Gradient Descent (SGD) just picks a random instance in the training set at every step and computes the gradients based only on that single instance. Obviously this makes the algorithm much faster since it has very little data to manipulate at every iteration. It also makes it possible to train on huge training sets, since only one instance needs to be in memory at each iteration.\n",
    "\n",
    "On the other hand, due to its stochastic (i.e., random) nature, this algorithm is much less regular than Batch Gradient Descent. Instead of gently decreasing until it reaches the minimum, the cost function will bounce up and down, decreasing only on average. Over time it will end up very close to the minimum, but once it gets there it will continue to bounce around, never settling down. So once the algorithm stops, the final parameter values are good, but not optimal.\n",
    "\n",
    "\n",
    "## Implementation Issues:\n",
    "\n",
    "We need to consider the following two issues for implementing gradient descent algorithms.\n",
    "\n",
    "####  Learning Rate\n",
    "When implementing the Gradient Descent algorithm, if the learning rate is set to a too low value, the algorithm will eventually reach the solution, but it will take a long time. On the other hand, if the learning rate is too high, the algorithm diverges, jumping all over the place and actually getting further and further away from the solution at every step.\n",
    "\n",
    "To find a good learning rate, we can use grid search. However, we may want to limit the number of iterations so that grid search can eliminate models that take too long to converge.\n",
    "\n",
    "\n",
    "#### No. of Iterations\n",
    "Setting the number of iterations is tricky. If it is too low, we will still be far away from the optimal solution when the algorithm stops. But if it is too high, we will waste time while the model parameters do not change anymore. A simple solution is to set a very large number of iterations but to interrupt the algorithm when the gradient vector becomes small. That is, when its norm becomes smaller than a tiny number $\\epsilon$ (called the tolerance). This happens when Gradient Descent has (almost) reached the minimum.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "URL: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html#sklearn.datasets.load_boston\n",
    "\n",
    "This dataset provides housing values in the suburbs of Boston.\n",
    "\n",
    "The **MEDV** variable is the target variable.\n",
    "\n",
    "### Data description\n",
    "\n",
    "The Boston data frame has 506 rows and 14 columns.\n",
    "\n",
    "This data frame contains the following columns:\n",
    "\n",
    "- CRIM: per capita crime rate by town.\n",
    "\n",
    "- ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "\n",
    "- INDUS: proportion of non-retail business acres per town.\n",
    "\n",
    "- CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
    "\n",
    "- NOX: nitrogen oxides concentration (parts per 10 million).\n",
    "\n",
    "- RM: average number of rooms per dwelling.\n",
    "\n",
    "- AGE: proportion of owner-occupied units built prior to 1940.\n",
    "\n",
    "- DIS: weighted mean of distances to five Boston employment centres.\n",
    "\n",
    "- RAD: index of accessibility to radial highways.\n",
    "\n",
    "- TAX: full-value property-tax rate per $10,000.\n",
    "\n",
    "- PTRATIO: pupil-teacher ratio by town.\n",
    "\n",
    "- B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n",
    "\n",
    "- LSTAT: lower status of the population (percent).\n",
    "\n",
    "- MEDV: median value of owner-occupied homes in $1000s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "First load the data and explore the feature names, target names, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13) (506,)\n",
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "boston = datasets.load_boston()\n",
    "print(boston.data.shape, boston.target.shape)\n",
    "print(boston.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create A DataFrame Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(boston.data,columns=boston.feature_names)\n",
    "df = pd.concat([df,pd.Series(boston.target,name='MEDV')],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Check of the Data\n",
    "\n",
    "Let’s take a look at the top five rows using the DataFrame’s head() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the Data\n",
    "\n",
    "DataFrame’s info() method is useful to get a quick description of the data, in particular the total number of rows, and each attribute’s type and number of non-null values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      "CRIM       506 non-null float64\n",
      "ZN         506 non-null float64\n",
      "INDUS      506 non-null float64\n",
      "CHAS       506 non-null float64\n",
      "NOX        506 non-null float64\n",
      "RM         506 non-null float64\n",
      "AGE        506 non-null float64\n",
      "DIS        506 non-null float64\n",
      "RAD        506 non-null float64\n",
      "TAX        506 non-null float64\n",
      "PTRATIO    506 non-null float64\n",
      "B          506 non-null float64\n",
      "LSTAT      506 non-null float64\n",
      "MEDV       506 non-null float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 55.4 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Data: Describe Numerical Attributes\n",
    "\n",
    "DataFrame's describe() method shows a summary of the numerical attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.613524</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.601545</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.677083</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.677083   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT        MEDV  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking for Numerical Correlations with the Target Column\n",
    "\n",
    "Since the dataset is not too large, we can easily compute the standard correlation coefficient (also called Pearson’s r) between every pair of attributes using DataFrame's corr() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MEDV       1.000000\n",
       "RM         0.695360\n",
       "ZN         0.360445\n",
       "B          0.333461\n",
       "DIS        0.249929\n",
       "CHAS       0.175260\n",
       "AGE       -0.376955\n",
       "RAD       -0.381626\n",
       "CRIM      -0.388305\n",
       "NOX       -0.427321\n",
       "TAX       -0.468536\n",
       "INDUS     -0.483725\n",
       "PTRATIO   -0.507787\n",
       "LSTAT     -0.737663\n",
       "Name: MEDV, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variable Correlations with the target \"MEDV\"\n",
    "\n",
    "df.corr()['MEDV'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Separate Feature Set (Data Matrix X) and Target (1D Vector y)\n",
    "\n",
    "Create a data matrix (X) that contains all features and a 1D target vector (y) containing the target.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "# It contains all attributes (all features and the target)\n",
    "allData = df\n",
    "\n",
    "\n",
    "# Use the following code to select ALL features\n",
    "X = df.drop(columns='MEDV')  # Data Matrix containing all features excluding the target\n",
    "\n",
    "\n",
    "y = df['MEDV'] # 1D targer vector\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale The Features\n",
    "\n",
    "We should ensure that all features have a similar scale. Otherwise optimization algorithms (e.g., Gradient Descent based algorithms) will take much longer time to converge.\n",
    "\n",
    "Also, regularization techniques are sensitive to the scale of data. Thus, we must scale the features before applying regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Sklearn's Stochastic Gradient Descent (SGD)\n",
    "\n",
    "\n",
    "Sklearn provides a SGDRegressor model. We will implement the regularized SGDRegressor.\n",
    "\n",
    "\n",
    "### Regularization\n",
    "\n",
    "Regularization is an effective technique for reducing overfitting.\n",
    "\n",
    "For a linear model, regularization is typically achieved by constraining the weights of the model. We will now look at three different ways to constrain the weights.\n",
    "\n",
    "- Ridge Regression ($l_2$ norm)\n",
    "- Lasso Regression ($l_1$ norm)\n",
    "- Elastic Net (it combines $l_1$ and $l_2$ priors as regularizer)\n",
    "\n",
    "\n",
    "### Regularization: A Common Hyperparameter\n",
    "\n",
    "In all regularization methods, we need to use the hyperparameter $\\alpha$. It controls how much we want to regularize the model. \n",
    "\n",
    "- If $\\alpha = 0$ then Ridge Regression is just Linear Regression. \n",
    "- If $\\alpha$ is very large, then all weights end up very close to zero and the result is a flat line going through the data’s mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "We will use two evaluation metrics.\n",
    "\n",
    "- Mean Squared Error (MSE)\n",
    "- Coefficient of Determination or $R^2$/$r^2$\n",
    "\n",
    "\n",
    "### Note on $R^2$:\n",
    "R-squared is a statistical measure of how close the data are to the fitted regression line. \n",
    "\n",
    "R-squared measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n",
    "\n",
    "R-squared = Explained variation / Total variation\n",
    "\n",
    "R-squared is always between 0 and 100%:\n",
    "\n",
    "- 0% indicates that the model explains none of the variability of the response data around its mean.\n",
    "- 100% indicates that the model explains all the variability of the response data around its mean.\n",
    "\n",
    "In general, the higher the R-squared, the better the model fits your data. \n",
    "\n",
    "#### Compute $R^2$ using the sklearn:\n",
    "\n",
    "- The \"score\" function of the OLS Linear Regression object\n",
    "- The \"r2_score\" function from sklearn.metrics\n",
    "\n",
    "#### Compute MSE using the sklearn:\n",
    "\n",
    "- The \"mean_squared_error\" function from sklearn.metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Regressor\n",
    "\n",
    "\n",
    "\n",
    "In the Stochastic Gradient Descent algorithm the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate).\n",
    "\n",
    "The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm $l_2$ or the absolute norm $l_1$ or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.\n",
    "\n",
    "\n",
    "We need to set the following hyperparameters.\n",
    "\n",
    "\n",
    "- penalty : str, ‘none’, ‘l2’, ‘l1’, or ‘elasticnet’\n",
    "\n",
    "The penalty (aka regularization term) to be used. Defaults to ‘l2’ which is the standard regularizer for linear SVM models. ‘l1’ and ‘elasticnet’ might bring sparsity to the model (feature selection) not achievable with ‘l2’.\n",
    "\n",
    "- alpha : float\n",
    "\n",
    "Constant that multiplies the regularization term. Defaults to 0.0001 Also used to compute learning_rate when set to ‘optimal’.\n",
    "\n",
    "- l1_ratio : float\n",
    "\n",
    "The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Defaults to 0.15.\n",
    "\n",
    "- learning_rate : string, optional\n",
    "\n",
    "The learning rate schedule:\n",
    "\n",
    "-- ‘constant’:\n",
    "eta = eta0\n",
    "\n",
    "-- ‘optimal’:\n",
    "eta = 1.0 / (alpha * (t + t0)) where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
    "\n",
    "-- ‘invscaling’: [default]\n",
    "eta = eta0 / pow(t, power_t)\n",
    "\n",
    "- eta0 : double\n",
    "\n",
    "The initial learning rate for the ‘constant’ or ‘invscaling’ schedules. The default value is 0.0 as eta0 is not used by the default schedule ‘optimal’.\n",
    "\n",
    "\n",
    "- max_iter : int, optional\n",
    "\n",
    "The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the fit method, and not the partial_fit. Defaults to 5. \n",
    "\n",
    "\n",
    "- tol : float or None, optional\n",
    "\n",
    "The stopping criterion. If it is not None, the iterations will stop when (loss > previous_loss - tol). Defaults to None. Defaults to 1e-3 from 0.21.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Implement SGD on a Linear Regression Model\n",
    "\n",
    "First we implement a SGD Linear Regression model without hyperparameter tuning. We do this first just to get a sense of the model's performance. Then, we perform hyperparameter tuning to determine optimal values for the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: \n",
      " [22.78652701]\n",
      "Coefficients: \n",
      " [-0.92241108  0.51371146  0.01958236  0.72903868 -1.75432889  3.19350245\n",
      " -0.17296238 -2.81189611  1.45094904 -1.01407278 -1.97814575  1.12098063\n",
      " -3.52327863]\n",
      "Number of Iterations: \n",
      " 30\n",
      "\n",
      "----------------------------- Model Evaluation -----------------------------\n",
      "Training: Mean squared error: 21.76\n",
      "Test: Mean squared error: 24.77\n",
      "\n",
      "Training: Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.75\n",
      "Test: Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.66\n"
     ]
    }
   ],
   "source": [
    "# SGD Regression\n",
    "\n",
    "# Create SGDRegressor linear regression object\n",
    "lin_reg_sgd = SGDRegressor(max_iter=1000, eta0=0.01, penalty=\"elasticnet\", l1_ratio=0.0, alpha=0.01)\n",
    "\n",
    "# Train the model\n",
    "lin_reg_sgd.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# The intercept\n",
    "print(\"Intercept: \\n\", lin_reg_sgd.intercept_)\n",
    "\n",
    "# The coefficients\n",
    "print(\"Coefficients: \\n\", lin_reg_sgd.coef_)\n",
    "\n",
    "# The number of iterations\n",
    "print(\"Number of Iterations: \\n\", lin_reg_sgd.n_iter_)\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Model Evaluation -----------------------------\")\n",
    "\n",
    "# Make prediction \n",
    "y_train_predicted_sgd = lin_reg_sgd.predict(X_train)\n",
    "y_test_predicted_sgd = lin_reg_sgd.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Training: Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train, y_train_predicted_sgd))\n",
    "\n",
    "print(\"Test: Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_test_predicted_sgd))\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"\\nTraining: Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % \n",
    "      r2_score(y_train, y_train_predicted_sgd))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Test: Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % \n",
    "      r2_score(y_test, y_test_predicted_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection: Hyperparameter Tunining\n",
    "\n",
    "\n",
    "A regression model is defined by a set of parameters: alpha, l1_ratio, etc. These are called hyperparameters.\n",
    "\n",
    "We need to select the best model based on the optimal values of these hyperparameters. This process is called hyperparameter tuning.\n",
    "\n",
    "The best way to do hyperparameter tuning is to use **cross-validation**.\n",
    "\n",
    "We will use Scikit-Learn’s GridSearchCV to search the combinations of hyperparameter values that provide best performance.\n",
    "\n",
    "We need to tell which hyperparameters we want the GridSearchCV to experiment with, and what values to try out. It will evaluate all the possible combinations of hyperparameter values, using cross-validation. \n",
    "\n",
    "\n",
    "### Important:\n",
    "\n",
    "The GridSearchCV takes an argument to define the scoring metric (performance measure). \n",
    "\n",
    "See the list of possible scoring functions:\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "For regression, we may use \"neg_mean_squared_error\" or \"explained_variance\" scoring function. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression: Hyperparameter Tuning for SGD Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1152 candidates, totalling 11520 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1452 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 3124 tasks      | elapsed:   16.7s\n",
      "[Parallel(n_jobs=-1)]: Done 5388 tasks      | elapsed:   24.8s\n",
      "[Parallel(n_jobs=-1)]: Done 9468 tasks      | elapsed:   33.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (negative mean squared error): -23.776077\n",
      "Optimal Hyperparameter Values:  {'alpha': 0.01, 'eta0': 0.01, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'loss': 'squared_loss', 'max_iter': 10000}\n",
      "\n",
      "\n",
      "CPU times: user 11.2 s, sys: 358 ms, total: 11.6 s\n",
      "Wall time: 38.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 11520 out of 11520 | elapsed:   38.6s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# The param_grid tells Scikit-Learn to evaluate all combinations of the hyperparameter values\n",
    "param_grid = {'alpha': [0.1, 0.01, 0.001], 'learning_rate': [\"constant\", \"optimal\", \"invscaling\"], \n",
    "              'l1_ratio': [1, 0.5, 0.2, 0], 'max_iter':[100, 400, 1000, 10000],'eta0': [0.01, 0.001],\n",
    "              'loss': ['squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']}\n",
    "\n",
    "\n",
    "\n",
    "sgd = SGDRegressor()\n",
    "\n",
    "sgd_cv = GridSearchCV(sgd, param_grid, scoring='neg_mean_squared_error', cv=10, verbose=2, n_jobs=-1)\n",
    "sgd_cv.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "params_optimal_sgd = sgd_cv.best_params_\n",
    "\n",
    "print(\"Best Score (negative mean squared error): %f\" % sgd_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal_sgd)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression: Select The Best Model for the SGD Regressor\n",
    "\n",
    "Using the optimal hyperparameter values, create the best model.\n",
    "Then, fit the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: \n",
      " [22.84544931]\n",
      "Coefficients: \n",
      " [-0.9022      0.52711153  0.12695643  0.69354744 -1.83666141  3.16125689\n",
      " -0.10066126 -3.03521397  1.77972575 -1.1865263  -1.97955591  1.10732345\n",
      " -3.50102086]\n",
      "Number of Iterations: \n",
      " 95\n",
      "\n",
      "----------------------------- Model Evaluation -----------------------------\n",
      "Mean squared error: 21.89\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.75\n"
     ]
    }
   ],
   "source": [
    "# SGD Regression\n",
    "\n",
    "# Create SGDRegressor linear regression object using the optimal hyperparameter values\n",
    "lin_reg_sgd = SGDRegressor(**params_optimal_sgd)\n",
    "\n",
    "# Train the model\n",
    "lin_reg_sgd.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# The intercept\n",
    "print(\"Intercept: \\n\", lin_reg_sgd.intercept_)\n",
    "\n",
    "# The coefficients\n",
    "print(\"Coefficients: \\n\", lin_reg_sgd.coef_)\n",
    "\n",
    "# The number of iterations\n",
    "print(\"Number of Iterations: \\n\", lin_reg_sgd.n_iter_)\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Model Evaluation -----------------------------\")\n",
    "\n",
    "# Make prediction \n",
    "y_train_predicted_sgd = lin_reg_sgd.predict(X_train)\n",
    "\n",
    "\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train, y_train_predicted_sgd))\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % r2_score(y_train, y_train_predicted_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression: Evaluate Model Performance for SGD Regressor\n",
    "\n",
    "Evaluate the model's performance using cross-validation. \n",
    "\n",
    "Use Scikit-Learn's cross_val_score function. \n",
    "\n",
    "Note that the \"scoring\" argument should be set based on the type of classification (binary/multiclass)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-14.74784841 -17.93447856 -28.46053203 -46.01235169 -21.53966764\n",
      " -26.48782046 -18.83904573 -20.53088499 -14.94533691 -32.80433796]\n",
      "Negative Mean Squared Error: -24.23 (+/- 18.29)\n"
     ]
    }
   ],
   "source": [
    "# Scoring Parameter for Regression:\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "scores = cross_val_score(lin_reg_sgd, X_train, y_train, scoring='neg_mean_squared_error', cv=10)\n",
    "print(scores)\n",
    "\n",
    "print(\"Negative Mean Squared Error: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression: Evaluate Model Performance Using Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 24.47\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Make prediction using the test data\n",
    "y_test_predicted = lin_reg_sgd.predict(X_test)\n",
    "\n",
    "\n",
    "test_mse_linear = mean_squared_error(y_test, y_test_predicted)\n",
    "\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % test_mse_linear)\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "test_r2_linear = r2_score(y_test, y_test_predicted)\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" \n",
    "      % test_r2_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement SGD on a Polynomial Regression Model\n",
    "\n",
    "We will use optimal polynomial degree obtained from previous notebook on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Original Features:  13\n",
      "No. of Augmented Features:  104\n"
     ]
    }
   ],
   "source": [
    "# Variable that speficies the degree of the polynomial to be added to the feature vector\n",
    "poly_degree = 2\n",
    "\n",
    "\n",
    "# Add polynomial terms with the feature vector using the sklearn PolynomialFeatures class\n",
    "poly_features = PolynomialFeatures(degree=poly_degree, include_bias=False)\n",
    "X_train_poly = poly_features.fit_transform(X_train)\n",
    "\n",
    "\n",
    "print(\"No. of Original Features: \", X_train.shape[1])\n",
    "print(\"No. of Augmented Features: \", X_train_poly.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression: Hyperparameter Tuning for SGD Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 384 candidates, totalling 3840 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 214 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1400 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1926 tasks      | elapsed:   27.3s\n",
      "[Parallel(n_jobs=-1)]: Done 3606 tasks      | elapsed:   48.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (negative mean squared error): -14.951719\n",
      "Optimal Hyperparameter Values:  {'alpha': 0.1, 'eta0': 0.001, 'l1_ratio': 0, 'learning_rate': 'invscaling', 'loss': 'squared_epsilon_insensitive', 'max_iter': 10000}\n",
      "\n",
      "\n",
      "CPU times: user 5.42 s, sys: 489 ms, total: 5.9 s\n",
      "Wall time: 55.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 3840 out of 3840 | elapsed:   55.1s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# The param_grid tells Scikit-Learn to evaluate all combinations of the hyperparameter values\n",
    "# param_grid = {'alpha': [0.1, 0.01], 'learning_rate': [\"invscaling\"], \n",
    "#               'l1_ratio': [1, 0.5, 0.2, 0], 'max_iter':[100, 400, 1000, 5000],'eta0': [0.01, 0.001, 0.0001]}\n",
    "\n",
    "\n",
    "param_grid = {'alpha': [0.1, 0.01], 'learning_rate': [\"invscaling\"], \n",
    "              'l1_ratio': [1, 0.5, 0.2, 0], 'max_iter':[100, 400, 1000, 10000],'eta0': [0.01, 0.001, 0.0001],\n",
    "              'loss': ['squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']}\n",
    "\n",
    "\n",
    "\n",
    "sgd = SGDRegressor()\n",
    "\n",
    "sgd_cv = GridSearchCV(sgd, param_grid, scoring='neg_mean_squared_error', cv=10, verbose=2, n_jobs=-1)\n",
    "sgd_cv.fit(X_train_poly, y_train)\n",
    "\n",
    "\n",
    "params_optimal_sgd = sgd_cv.best_params_\n",
    "\n",
    "print(\"Best Score (negative mean squared error): %f\" % sgd_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", params_optimal_sgd)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression: Select The Best Model for the SGD Regressor\n",
    "\n",
    "Using the optimal hyperparameter values, create the best model.\n",
    "Then, fit the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Iterations: \n",
      " 401\n",
      "\n",
      "----------------------------- Model Evaluation -----------------------------\n",
      "Mean squared error: 8.02\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.91\n"
     ]
    }
   ],
   "source": [
    "# SGD Regression\n",
    "\n",
    "# Create SGDRegressor linear regression object using the optimal hyperparameter values\n",
    "lin_reg_sgd = SGDRegressor(**params_optimal_sgd)\n",
    "\n",
    "# Train the model\n",
    "lin_reg_sgd.fit(X_train_poly, y_train)\n",
    "\n",
    "\n",
    "# # The intercept\n",
    "# print(\"Intercept: \\n\", lin_reg_sgd.intercept_)\n",
    "\n",
    "# # The coefficients\n",
    "# print(\"Coefficients: \\n\", lin_reg_sgd.coef_)\n",
    "\n",
    "# The number of iterations\n",
    "print(\"Number of Iterations: \\n\", lin_reg_sgd.n_iter_)\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Model Evaluation -----------------------------\")\n",
    "\n",
    "# Make prediction \n",
    "y_train_predicted_sgd = lin_reg_sgd.predict(X_train_poly)\n",
    "\n",
    "\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train, y_train_predicted_sgd))\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % r2_score(y_train, y_train_predicted_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression: Evaluate Model Performance Using Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 14.33\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.80\n"
     ]
    }
   ],
   "source": [
    "poly_features = PolynomialFeatures(degree=poly_degree, include_bias=False)\n",
    "X_test_poly = poly_features.fit_transform(X_test)\n",
    "\n",
    "# Make prediction using the test data\n",
    "y_test_predicted = lin_reg_sgd.predict(X_test_poly)\n",
    "\n",
    "test_mse_polynomial = mean_squared_error(y_test, y_test_predicted)\n",
    "\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % test_mse_polynomial)\n",
    "\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "\n",
    "test_r2_polynomial = r2_score(y_test, y_test_predicted)\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" \n",
    "      % test_r2_polynomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Linear Regression vs. SGD Polynomial Regression \n",
    "\n",
    "We observe that the SGD Polynomial Regression performs significantly better. It uses regularization to reduce the overfitting problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>SGD Linear Regression</th>\n",
       "      <th>SGD Polynomial Regression (degree 2)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSE (test)</td>\n",
       "      <td>24.474214</td>\n",
       "      <td>14.32867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R2 Score (test)</td>\n",
       "      <td>0.666263</td>\n",
       "      <td>0.80461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Metric  SGD Linear Regression  \\\n",
       "0       MSE (test)              24.474214   \n",
       "1  R2 Score (test)               0.666263   \n",
       "\n",
       "   SGD Polynomial Regression (degree 2)  \n",
       "0                              14.32867  \n",
       "1                               0.80461  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[\"MSE (test)\", test_mse_linear, test_mse_polynomial], \n",
    "        [\"R2 Score (test)\", test_r2_linear, test_r2_polynomial]]\n",
    "pd.DataFrame(data, columns=[\"Metric\", \"SGD Linear Regression\", \"SGD Polynomial Regression (degree 2)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond Linear Regression\n",
    "\n",
    "Based on all experiments that we have done so far, we observe that even the optimal Linear Regression model (**regularized polynomial regression of degree 2**, implemented in notebook 3) is unable to reduce the MSE below 12. \n",
    "\n",
    "To further reduce the MSE we will have to use more sophisticated regression models. Below we apply two advanced regressors on the same dataset:\n",
    "- Support Vector Machine (Gaussian Radial Basis Function)\n",
    "- Random Forest\n",
    "\n",
    "We did not fine tune the hyperparameters. Please refer to the notebooks on these two models for a detail discussion on their hyperparameters. Below we use empirically tested optimal values for the hyperparameters. Our goal is to illustrate the superiority of these advanced models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (Gaussian Radial Basis Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------- Model Evaluation -----------------------------\n",
      "Train: Mean squared error: 5.71\n",
      "Train: Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.93\n",
      "\n",
      "Test: Mean squared error: 10.63\n",
      "Test: Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.86\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "svm = SVR(kernel='rbf', C=1000.0, gamma=0.01)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Model Evaluation -----------------------------\")\n",
    "\n",
    "# Make prediction \n",
    "y_train_predicted_svm = svm.predict(X_train)\n",
    "\n",
    "\n",
    "print(\"Train: Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train, y_train_predicted_svm))\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Train: Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % r2_score(y_train, y_train_predicted_svm))\n",
    "\n",
    "\n",
    "# Make prediction using the test data\n",
    "y_test_predicted_svm = svm.predict(X_test)\n",
    "\n",
    "test_mse_svm = mean_squared_error(y_test, y_test_predicted_svm)\n",
    "\n",
    "print(\"\\nTest: Mean squared error: %.2f\"\n",
    "      % test_mse_svm)\n",
    "\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "\n",
    "test_r2_svm = r2_score(y_test, y_test_predicted_svm)\n",
    "print(\"Test: Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" \n",
    "      % test_r2_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "To use the Random Forest model we need the unscaled features. Thus we read the features from the DataFrame object first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "# Use the following code to select ALL features\n",
    "X = allData.drop(columns='MEDV')  # Data Matrix containing all features excluding the target\n",
    "\n",
    "\n",
    "y = allData['MEDV'] # 1D targer vector\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "# Create train and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Mean squared error: 2.75\n",
      "Train: Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.97\n",
      "Test: Mean squared error: 8.48\n",
      "Test: Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "rnd_forest_reg = RandomForestRegressor(n_estimators=500, criterion=\"mse\", max_features=\"auto\", \n",
    "                                       verbose=1, max_depth=8, \n",
    "                                       oob_score=True, n_jobs=-1)\n",
    "\n",
    "rnd_forest_reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Make prediction \n",
    "y_train_predicted_rnd_forest = rnd_forest_reg.predict(X_train)\n",
    "\n",
    "\n",
    "train_mse_rnd_forest = mean_squared_error(y_train, y_train_predicted_rnd_forest)\n",
    "\n",
    "print(\"Train: Mean squared error: %.2f\"\n",
    "      % train_mse_rnd_forest)\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Train: Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" \n",
    "      % r2_score(y_train, y_train_predicted_rnd_forest))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_test_predicted_rnd_forest = rnd_forest_reg.predict(X_test)\n",
    "\n",
    "\n",
    "test_mse_rnd_forest = mean_squared_error(y_test, y_test_predicted_rnd_forest)\n",
    "\n",
    "print(\"Test: Mean squared error: %.2f\"\n",
    "      % test_mse_rnd_forest)\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "\n",
    "test_r2_rnd_forest = r2_score(y_test, y_test_predicted_rnd_forest)\n",
    "\n",
    "\n",
    "print(\"Test: Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" \n",
    "      % test_r2_rnd_forest)\n",
    "\n",
    "#print(\"\\nScore of the training dataset obtained using an out-of-bag estimate: \", rnd_forest_reg.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Linear Regression & Advanced Regressor Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MSE (test)</th>\n",
       "      <th>R2 Score (test)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD Linear Regression</td>\n",
       "      <td>24.474214</td>\n",
       "      <td>0.666263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SGD Polynomial Regression (degree 2)</td>\n",
       "      <td>14.328670</td>\n",
       "      <td>0.804610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Support Vector Machine (Gaussian RBF)</td>\n",
       "      <td>10.627752</td>\n",
       "      <td>0.855077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>8.482888</td>\n",
       "      <td>0.884325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Model  MSE (test)  R2 Score (test)\n",
       "0                  SGD Linear Regression   24.474214         0.666263\n",
       "1   SGD Polynomial Regression (degree 2)   14.328670         0.804610\n",
       "2  Support Vector Machine (Gaussian RBF)   10.627752         0.855077\n",
       "3                          Random Forest    8.482888         0.884325"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "        [\"SGD Linear Regression\", test_mse_linear, test_r2_linear], \n",
    "        [\"SGD Polynomial Regression (degree 2)\", test_mse_polynomial, test_r2_polynomial],\n",
    "        [\"Support Vector Machine (Gaussian RBF)\", test_mse_svm, test_r2_svm],\n",
    "        [\"Random Forest\", test_mse_rnd_forest, test_r2_rnd_forest]\n",
    "       ]\n",
    "\n",
    "\n",
    "pd.DataFrame(data, columns=[\"Model\", \"MSE (test)\", \"R2 Score (test)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Observation\n",
    "\n",
    "We observe that Random Forest performs significantly better than Linear Regression reducing the MSE below 9."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
