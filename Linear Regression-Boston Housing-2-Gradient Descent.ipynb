{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression - Boston Housing Dataset - Gradient Descent\n",
    "\n",
    "We will perform linear regression using the sklearn's Gradient Descent model.\n",
    "\n",
    "### Gradient Descent\n",
    "The gradient descent based methods are used when there are a large number of features, or too many training instances to fit in memory.\n",
    "\n",
    "There are three variants of the Gradient Descent Algorithms.\n",
    "- Batch Gradient Descent\n",
    "- Stochastic Gradient Descent (SGD)\n",
    "- Mini-batch Gradient Descent\n",
    "\n",
    "Scikit-Learn provides only the Stochastic Gradient Descent model. In this notebook, we will see how to use sklearn's SGDRegressor.\n",
    "\n",
    "\n",
    "#### Batch Gradient Descent\n",
    "Batch Gradient Descent uses the whole batch of training data at every step. As a result it is terribly slow on very large training sets.\n",
    "\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "\n",
    "The main problem with Batch Gradient Descent is the fact that it uses the whole training set to compute the gradients at every step, which makes it very slow when the training set is large. At the opposite extreme, Stochastic Gradient Descent (SGD) just picks a random instance in the training set at every step and computes the gradients based only on that single instance. Obviously this makes the algorithm much faster since it has very little data to manipulate at every iteration. It also makes it possible to train on huge training sets, since only one instance needs to be in memory at each iteration.\n",
    "\n",
    "On the other hand, due to its stochastic (i.e., random) nature, this algorithm is much less regular than Batch Gradient Descent. Instead of gently decreasing until it reaches the minimum, the cost function will bounce up and down, decreasing only on average. Over time it will end up very close to the minimum, but once it gets there it will continue to bounce around, never settling down. So once the algorithm stops, the final parameter values are good, but not optimal.\n",
    "\n",
    "\n",
    "## Implementation Issues:\n",
    "\n",
    "We need to consider the following two issues for implementing gradient descent algorithms.\n",
    "\n",
    "####  Learning Rate\n",
    "When implementing the Gradient Descent algorithm, if the learning rate is set to a too low value, the algorithm will eventually reach the solution, but it will take a long time. On the other hand, if the learning rate is too high, the algorithm diverges, jumping all over the place and actually getting further and further away from the solution at every step.\n",
    "\n",
    "To find a good learning rate, we can use grid search. However, we may want to limit the number of iterations so that grid search can eliminate models that take too long to converge.\n",
    "\n",
    "\n",
    "#### No. of Iterations\n",
    "You may wonder how to set the number of iterations. If it is too low, you will still be far away from the optimal solution when the algorithm stops, but if it is too high, you will waste time while the model parameters do not change anymore. A simple solution is to set a very large number of iterations but to interrupt the algorithm when the gradient vector becomes small. That is, when its norm becomes smaller than a tiny number $\\epsilon$ (called the tolerance). This happens when Gradient Descent has (almost) reached the minimum.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "URL: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html#sklearn.datasets.load_boston\n",
    "\n",
    "This dataset provides housing values in the suburbs of Boston.\n",
    "\n",
    "The **MEDV** variable is the target variable.\n",
    "\n",
    "### Data description\n",
    "\n",
    "The Boston data frame has 506 rows and 14 columns.\n",
    "\n",
    "This data frame contains the following columns:\n",
    "\n",
    "- CRIM: per capita crime rate by town.\n",
    "\n",
    "- ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "\n",
    "- INDUS: proportion of non-retail business acres per town.\n",
    "\n",
    "- CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
    "\n",
    "- NOX: nitrogen oxides concentration (parts per 10 million).\n",
    "\n",
    "- RM: average number of rooms per dwelling.\n",
    "\n",
    "- AGE: proportion of owner-occupied units built prior to 1940.\n",
    "\n",
    "- DIS: weighted mean of distances to five Boston employment centres.\n",
    "\n",
    "- RAD: index of accessibility to radial highways.\n",
    "\n",
    "- TAX: full-value property-tax rate per $10,000.\n",
    "\n",
    "- PTRATIO: pupil-teacher ratio by town.\n",
    "\n",
    "- B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n",
    "\n",
    "- LSTAT: lower status of the population (percent).\n",
    "\n",
    "- MEDV: median value of owner-occupied homes in $1000s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "First load the data and explore the feature names, target names, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13) (506,)\n",
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "boston = datasets.load_boston()\n",
    "print(boston.data.shape, boston.target.shape)\n",
    "print(boston.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create A DataFrame Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(boston.data,columns=boston.feature_names)\n",
    "df = pd.concat([df,pd.Series(boston.target,name='MEDV')],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Check of the Data\n",
    "\n",
    "Let’s take a look at the top five rows using the DataFrame’s head() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the Data\n",
    "\n",
    "DataFrame’s info() method is useful to get a quick description of the data, in particular the total number of rows, and each attribute’s type and number of non-null values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      "CRIM       506 non-null float64\n",
      "ZN         506 non-null float64\n",
      "INDUS      506 non-null float64\n",
      "CHAS       506 non-null float64\n",
      "NOX        506 non-null float64\n",
      "RM         506 non-null float64\n",
      "AGE        506 non-null float64\n",
      "DIS        506 non-null float64\n",
      "RAD        506 non-null float64\n",
      "TAX        506 non-null float64\n",
      "PTRATIO    506 non-null float64\n",
      "B          506 non-null float64\n",
      "LSTAT      506 non-null float64\n",
      "MEDV       506 non-null float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 55.4 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Data: Describe Numerical Attributes\n",
    "\n",
    "DataFrame's describe() method shows a summary of the numerical attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.593761</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.596783</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.647423</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.593761   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.596783   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.647423   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT        MEDV  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking for Numerical Correlations with the Target Column\n",
    "\n",
    "Since the dataset is not too large, we can easily compute the standard correlation coefficient (also called Pearson’s r) between every pair of attributes using DataFrame's corr() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MEDV       1.000000\n",
       "RM         0.695360\n",
       "ZN         0.360445\n",
       "B          0.333461\n",
       "DIS        0.249929\n",
       "CHAS       0.175260\n",
       "AGE       -0.376955\n",
       "RAD       -0.381626\n",
       "CRIM      -0.385832\n",
       "NOX       -0.427321\n",
       "TAX       -0.468536\n",
       "INDUS     -0.483725\n",
       "PTRATIO   -0.507787\n",
       "LSTAT     -0.737663\n",
       "Name: MEDV, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variable Correlations with the target \"MEDV\"\n",
    "\n",
    "df.corr()['MEDV'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Separate Feature Set (Data Matrix X) and Target (1D Vector y)\n",
    "\n",
    "Create a data matrix (X) that contains all features and a 1D target vector (y) containing the target.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# It contains all attributes (all features and the target)\n",
    "allData = df\n",
    "\n",
    "# Use the following code to select a subset of the features, e.g., \"LSTAT\"\n",
    "#X = df[['LSTAT', 'RM']]\n",
    "\n",
    "\n",
    "# Use the following code to select ALL features\n",
    "X = df.drop(columns='MEDV')  # Data Matrix containing all features excluding the target\n",
    "\n",
    "\n",
    "y = df['MEDV'] # 1D targer vector\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Scale The Features\n",
    "\n",
    "We should ensure that all features have a similar scale. Otherwise optimization algorithms (e.g., Gradient Descent based algorithms) will take much longer time to converge.\n",
    "\n",
    "Also, regularization techniques are sensitive to the scale of data. Thus, we must scale the features before applying regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Sklearn's Stochastic Gradient Descent (SGD)\n",
    "\n",
    "\n",
    "Sklearn provides a SGDRegressor model. We will implement regularized SGDRegressor.\n",
    "\n",
    "\n",
    "### Regularization\n",
    "\n",
    "Regularization is an effective technique for reducing overfitting.\n",
    "\n",
    "For a linear model, regularization is typically achieved by constraining the weights of the model. We will now look at three different ways to constrain the weights.\n",
    "\n",
    "- Ridge Regression ($l_2$ norm)\n",
    "- Lasso Regression ($l_1$ norm)\n",
    "- Elastic Net (it combines $l_1$ and $l_2$ priors as regularizer)\n",
    "\n",
    "\n",
    "### Regularization: A Common Hyperparameter\n",
    "\n",
    "In all regularization methods, we need to use the hyperparameter $\\alpha$. It controls how much we want to regularize the model. \n",
    "\n",
    "- If $\\alpha = 0$ then Ridge Regression is just Linear Regression. \n",
    "- If $\\alpha$ is very large, then all weights end up very close to zero and the result is a flat line going through the data’s mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "We will use two evaluation metrics.\n",
    "\n",
    "- Mean Squared Error (MSE)\n",
    "- Coefficient of Determination or $R^2$/$r^2$\n",
    "\n",
    "\n",
    "### Note on $R^2$:\n",
    "R-squared is a statistical measure of how close the data are to the fitted regression line. \n",
    "\n",
    "R-squared measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n",
    "\n",
    "R-squared = Explained variation / Total variation\n",
    "\n",
    "R-squared is always between 0 and 100%:\n",
    "\n",
    "- 0% indicates that the model explains none of the variability of the response data around its mean.\n",
    "- 100% indicates that the model explains all the variability of the response data around its mean.\n",
    "\n",
    "In general, the higher the R-squared, the better the model fits your data. \n",
    "\n",
    "#### Compute $R^2$ using the sklearn:\n",
    "\n",
    "- The \"score\" function of the OLS Linear Regression object\n",
    "- The \"r2_score\" function from sklearn.metrics\n",
    "\n",
    "#### Compute MSE using the sklearn:\n",
    "\n",
    "- The \"mean_squared_error\" function from sklearn.metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Regressor\n",
    "\n",
    "\n",
    "\n",
    "In the Stochastic Gradient Descent algorithm the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate).\n",
    "\n",
    "The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm $l_2$ or the absolute norm $l_1$ or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.\n",
    "\n",
    "\n",
    "We need to set the following parameters.\n",
    "\n",
    "\n",
    "- penalty : str, ‘none’, ‘l2’, ‘l1’, or ‘elasticnet’\n",
    "\n",
    "The penalty (aka regularization term) to be used. Defaults to ‘l2’ which is the standard regularizer for linear SVM models. ‘l1’ and ‘elasticnet’ might bring sparsity to the model (feature selection) not achievable with ‘l2’.\n",
    "\n",
    "- alpha : float\n",
    "\n",
    "Constant that multiplies the regularization term. Defaults to 0.0001 Also used to compute learning_rate when set to ‘optimal’.\n",
    "\n",
    "- l1_ratio : float\n",
    "\n",
    "The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Defaults to 0.15.\n",
    "\n",
    "- learning_rate : string, optional\n",
    "\n",
    "The learning rate schedule:\n",
    "\n",
    "-- ‘constant’:\n",
    "eta = eta0\n",
    "\n",
    "-- ‘optimal’:\n",
    "eta = 1.0 / (alpha * (t + t0)) where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
    "\n",
    "-- ‘invscaling’: [default]\n",
    "eta = eta0 / pow(t, power_t)\n",
    "\n",
    "- eta0 : double\n",
    "\n",
    "The initial learning rate for the ‘constant’ or ‘invscaling’ schedules. The default value is 0.0 as eta0 is not used by the default schedule ‘optimal’.\n",
    "\n",
    "\n",
    "- max_iter : int, optional\n",
    "\n",
    "The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the fit method, and not the partial_fit. Defaults to 5. \n",
    "\n",
    "\n",
    "- tol : float or None, optional\n",
    "\n",
    "The stopping criterion. If it is not None, the iterations will stop when (loss > previous_loss - tol). Defaults to None. Defaults to 1e-3 from 0.21.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: \n",
      " [22.47917382]\n",
      "Coefficients: \n",
      " [-0.93016247  0.6285863   0.19324335  0.72083083 -1.87808132  3.12519536\n",
      " -0.17196692 -2.91606847  1.99092592 -1.51746321 -1.94254065  1.12427836\n",
      " -3.57317331]\n",
      "Number of Iterations: \n",
      " 1000\n",
      "\n",
      "----------------------------- Model Evaluation -----------------------------\n",
      "Mean squared error: 21.66\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.75\n"
     ]
    }
   ],
   "source": [
    "# SGD Regression\n",
    "\n",
    "# Create SGDRegressor linear regression object\n",
    "lin_reg_sgd = SGDRegressor(max_iter=1000, eta0=0.01, penalty=\"elasticnet\", l1_ratio=0.0, alpha=0.01)\n",
    "\n",
    "# Train the model\n",
    "lin_reg_sgd.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# The intercept\n",
    "print(\"Intercept: \\n\", lin_reg_sgd.intercept_)\n",
    "\n",
    "# The coefficients\n",
    "print(\"Coefficients: \\n\", lin_reg_sgd.coef_)\n",
    "\n",
    "# The number of iterations\n",
    "print(\"Number of Iterations: \\n\", lin_reg_sgd.n_iter_)\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Model Evaluation -----------------------------\")\n",
    "\n",
    "# Make prediction \n",
    "y_train_predicted_sgd = lin_reg_sgd.predict(X_train)\n",
    "\n",
    "\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train, y_train_predicted_sgd))\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % r2_score(y_train, y_train_predicted_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection: Hyperparameter Tunining\n",
    "\n",
    "\n",
    "A regression model is defined by a set of parameters: alpha, l1_ratio, etc. These are called hyperparameters.\n",
    "\n",
    "We need to select the best model based on the optimal values of these hyperparameters. This process is called hyperparameter tuning.\n",
    "\n",
    "The best way to do hyperparameter tuning is to use **cross-validation**.\n",
    "\n",
    "We will use Scikit-Learn’s GridSearchCV to search the combinations of hyperparameter values that provide best performance.\n",
    "\n",
    "We need to tell which hyperparameters we want the GridSearchCV to experiment with, and what values to try out. It will evaluate all the possible combinations of hyperparameter values, using cross-validation. \n",
    "\n",
    "\n",
    "### Important:\n",
    "\n",
    "The GridSearchCV takes an argument to define the scoring metric (performance measure). \n",
    "\n",
    "See the list of possible scoring functions:\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "For regression, we may use \"neg_mean_squared_error\" or \"explained_variance\" scoring function. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning for SGD Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (negative mean squared error): -23.770782\n",
      "Optimal Hyperparameter Values:  {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'optimal', 'l1_ratio': 1, 'max_iter': 1000}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'alpha': [0.1, 0.01, 0.001], 'learning_rate': [\"constant\", \"optimal\"], \n",
    "              'l1_ratio': [1, 0.5, 0.2, 0], 'max_iter':[100, 400, 1000, 10000],'eta0': [0.01, 0.001]}\n",
    "\n",
    "sgd = SGDRegressor()\n",
    "\n",
    "sgd_cv = GridSearchCV(sgd, param_grid, scoring='neg_mean_squared_error', cv=10)\n",
    "sgd_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Score (negative mean squared error): %f\" % sgd_cv.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", sgd_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select The Best Model for the SGD Regressor\n",
    "\n",
    "Using the optimal hyperparameter values, create the best model.\n",
    "Then, fit the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: \n",
      " [22.51649817]\n",
      "Coefficients: \n",
      " [-0.96257567  0.67141032  0.29814318  0.82784117 -1.95446417  3.10428137\n",
      " -0.11102958 -3.10408896  2.38239162 -1.7250197  -1.9421833   1.08660684\n",
      " -3.63522619]\n",
      "Number of Iterations: \n",
      " 1000\n",
      "\n",
      "----------------------------- Model Evaluation -----------------------------\n",
      "Mean squared error: 21.79\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.75\n"
     ]
    }
   ],
   "source": [
    "# SGD Regression\n",
    "\n",
    "# Create SGDRegressor linear regression object\n",
    "lin_reg_sgd = SGDRegressor(max_iter=1000, learning_rate=\"optimal\", eta0=0.01, penalty=\"elasticnet\", l1_ratio=1, alpha=0.001)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "lin_reg_sgd.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# The intercept\n",
    "print(\"Intercept: \\n\", lin_reg_sgd.intercept_)\n",
    "\n",
    "# The coefficients\n",
    "print(\"Coefficients: \\n\", lin_reg_sgd.coef_)\n",
    "\n",
    "# The number of iterations\n",
    "print(\"Number of Iterations: \\n\", lin_reg_sgd.n_iter_)\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Model Evaluation -----------------------------\")\n",
    "\n",
    "# Make prediction \n",
    "y_train_predicted_sgd = lin_reg_sgd.predict(X_train)\n",
    "\n",
    "\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train, y_train_predicted_sgd))\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % r2_score(y_train, y_train_predicted_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance for SGD Regressor\n",
    "\n",
    "Evaluate the model's performance using cross-validation. \n",
    "\n",
    "Use Scikit-Learn's cross_val_score function. \n",
    "\n",
    "Note that the \"scoring\" argument should be set based on the type of classification (binary/multiclass)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-13.27571271 -18.79160326 -29.38252744 -44.40325693 -21.66911309\n",
      " -26.55468302 -18.66933701 -20.83267471 -13.48382816 -33.27375323]\n",
      "Negative Mean Squared Error: -24.03 (+/- 18.27)\n"
     ]
    }
   ],
   "source": [
    "# Scoring Parameter for Regression:\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "scores = cross_val_score(lin_reg_sgd, X_train, y_train, scoring='neg_mean_squared_error', cv=10)\n",
    "print(scores)\n",
    "\n",
    "print(\"Negative Mean Squared Error: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance Using Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 24.20\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.67\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make prediction using the test data\n",
    "y_test_predicted = lin_reg_sgd.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_test_predicted))\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % r2_score(y_test, y_test_predicted))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
